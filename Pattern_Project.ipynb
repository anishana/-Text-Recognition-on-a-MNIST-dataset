{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pattern Project",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anishana/Text-Recognition-on-a-MNIST-dataset/blob/main/Pattern_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5lj-sbJS2kL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3154de03-86d6-4533-dff1-290650a9bb1d"
      },
      "source": [
        "! [ ! -z \"$COLAB_GPU\" ] && pip install torch scikit-learn==0.20.* skorch\n",
        "!pip install scikit-learn --upgrade\n",
        "# Run this when opening the notebook for the first time"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
            "Collecting scikit-learn==0.20.*\n",
            "  Downloading scikit_learn-0.20.4-cp37-cp37m-manylinux1_x86_64.whl (5.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.4 MB 727 kB/s \n",
            "\u001b[?25hCollecting skorch\n",
            "  Downloading skorch-0.11.0-py3-none-any.whl (155 kB)\n",
            "\u001b[K     |████████████████████████████████| 155 kB 50.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.20.*) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.20.*) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.10.0.2)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.7/dist-packages (from skorch) (0.8.9)\n",
            "Requirement already satisfied: tqdm>=4.14.0 in /usr/local/lib/python3.7/dist-packages (from skorch) (4.62.3)\n",
            "Installing collected packages: scikit-learn, skorch\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.1\n",
            "    Uninstalling scikit-learn-1.0.1:\n",
            "      Successfully uninstalled scikit-learn-1.0.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "imbalanced-learn 0.8.1 requires scikit-learn>=0.24, but you have scikit-learn 0.20.4 which is incompatible.\u001b[0m\n",
            "Successfully installed scikit-learn-0.20.4 skorch-0.11.0\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (0.20.4)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.0.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (23.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 23.2 MB 1.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (3.0.0)\n",
            "Installing collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 0.20.4\n",
            "    Uninstalling scikit-learn-0.20.4:\n",
            "      Successfully uninstalled scikit-learn-0.20.4\n",
            "Successfully installed scikit-learn-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_7THc2yWa3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "852163c0-6388-4729-e23e-75523798a74c"
      },
      "source": [
        "# Run this to mount your drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGjP4jwuT-Pu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4c464b5-9304-4774-a9d0-0bac0085d5e2"
      },
      "source": [
        "# Noise generation code\n",
        "# This code generates the six types of dataset and stores the results in a dictionary for easy access\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "from keras.utils import np_utils\n",
        "from numpy.testing import assert_array_almost_equal\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Set random seed\n",
        "np.random.seed(123)\n",
        "\n",
        "\n",
        "def other_class(n_classes, current_class):\n",
        "    \"\"\"\n",
        "    Returns a list of class indices excluding the class indexed by class_ind\n",
        "    :param nb_classes: number of classes in the task\n",
        "    :param class_ind: the class index to be omitted\n",
        "    :return: one random class that != class_ind\n",
        "    \"\"\"\n",
        "    if current_class < 0 or current_class >= n_classes:\n",
        "        error_str = \"class_ind must be within the range (0, nb_classes - 1)\"\n",
        "        raise ValueError(error_str)\n",
        "\n",
        "    other_class_list = list(range(n_classes))\n",
        "    other_class_list.remove(current_class)\n",
        "    other_class = np.random.choice(other_class_list)\n",
        "    return other_class\n",
        "\n",
        "def get_data(asym=False,balance = False,plain = False,determ = False):\n",
        "    \"\"\"\n",
        "    Get training images with specified ratio of syn/ayn label noise\n",
        "    \"\"\"\n",
        "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "    if(balance):\n",
        "        smote = SMOTE(random_state=42)\n",
        "        X_train = X_train.reshape(60000,784)\n",
        "        X_train, y_train = smote.fit_resample(X_train,y_train)\n",
        "\n",
        "    X_train = X_train.reshape(-1, 28, 28, 1)\n",
        "    X_test = X_test.reshape(-1, 28, 28, 1)\n",
        "\n",
        "    X_train = X_train / 255.0\n",
        "    X_test = X_test / 255.0\n",
        "\n",
        "\n",
        "    X_train = X_train.astype('float32')\n",
        "    X_test = X_test.astype('float32')\n",
        "\n",
        "    y_train_clean = np.copy(y_train)\n",
        "\n",
        "    if(plain):\n",
        "      if(determ == False):\n",
        "        # Only apply one hot if we're going to use this on a DL type algorithm\n",
        "        y_train = np_utils.to_categorical(y_train, 10)\n",
        "        y_test = np_utils.to_categorical(y_test, 10)\n",
        "      print(\"X_train:\", X_train.shape)\n",
        "      print(\"y_train:\", y_train.shape)\n",
        "      print(\"X_test:\", X_test.shape)\n",
        "      print(\"y_test\", y_test.shape)\n",
        "      return X_train, y_train, X_test, y_test\n",
        "\n",
        "    # generate random noisy labels\n",
        "    if asym:\n",
        "        # 1 < - 7, 2 -> 7, 3 -> 8, 5 <-> 6\n",
        "        source_class = [7, 2, 3, 5, 6]\n",
        "        target_class = [1, 7, 8, 6, 5]\n",
        "        \n",
        "        for s, t in zip(source_class, target_class):\n",
        "            cls_idx = np.where(y_train_clean == s)[0]\n",
        "            # print('cls_idx',cls_idx)\n",
        "            n_noisy = int(40 * cls_idx.shape[0] / 100)\n",
        "            # print('n_noisy',n_noisy)\n",
        "            noisy_sample_index = np.random.choice(cls_idx, n_noisy, replace=False)\n",
        "            y_train[noisy_sample_index] = t\n",
        "            # print(y_train[noisy_sample_index])\n",
        "            # print(y_train_clean[noisy_sample_index])\n",
        "    \n",
        "    else:\n",
        "        n_samples = y_train.shape[0]\n",
        "        n_noisy = int(40 * n_samples / 100)\n",
        "        class_index = [np.where(y_train_clean == i)[0] for i in range(10)]\n",
        "        class_noisy = int(n_noisy / 10)\n",
        "\n",
        "        noisy_idx = []\n",
        "        for d in range(10):\n",
        "            noisy_class_index = np.random.choice(class_index[d], class_noisy, replace=False)\n",
        "            noisy_idx.extend(noisy_class_index)\n",
        "            \n",
        "\n",
        "        for i in noisy_idx:\n",
        "            y_train[i] = other_class(n_classes=10, current_class=y_train[i])\n",
        "            # print(y_train[noisy_class_index])\n",
        "            # print(y_train_clean[noisy_class_index])\n",
        "\n",
        "\n",
        "        # print statistics\n",
        "        print(\"Print noisy label generation statistics:\")\n",
        "        for i in range(10):\n",
        "            n_noisy = np.sum(y_train == i)\n",
        "            print(\"Noisy class %s, has %s samples.\" % (i, n_noisy))\n",
        "\n",
        "\n",
        "    # one-hot-encode the labels\n",
        "    if(determ == False):\n",
        "      y_train = np_utils.to_categorical(y_train, 10)\n",
        "      y_test = np_utils.to_categorical(y_test, 10)\n",
        "\n",
        "    print(\"X_train:\", X_train.shape)\n",
        "    print(\"y_train:\", y_train.shape)\n",
        "    print(\"X_test:\", X_test.shape)\n",
        "    print(\"y_test\", y_test.shape)\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    list_of_filenames = ['imbalanced','balanced','imbalanced_asym', 'balanced_asym','imbalanced_sym','balanced_sym']\n",
        "    list_of_noisedata = [get_data(plain = True),get_data(balance = True, plain = True),get_data(asym=True),get_data(asym=True,balance  =True),get_data(),get_data(balance  =True)]\n",
        "    list_of_noisedata_deterministic = [get_data(plain = True,determ = True),get_data(balance = True, plain = True,determ = True),get_data(asym=True,determ = True),get_data(asym=True,balance  =True,determ = True),get_data(determ = True),get_data(balance  =True,determ = True)]\n",
        "    mapped_function_dict = dict(zip(list_of_filenames,list_of_noisedata))\n",
        "    mapped_function_dict_deterministic = dict(zip(list_of_filenames,list_of_noisedata_deterministic))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n",
            "X_train: (60000, 28, 28, 1)\n",
            "y_train: (60000, 10)\n",
            "X_test: (10000, 28, 28, 1)\n",
            "y_test (10000, 10)\n",
            "X_train: (67420, 28, 28, 1)\n",
            "y_train: (67420, 10)\n",
            "X_test: (10000, 28, 28, 1)\n",
            "y_test (10000, 10)\n",
            "X_train: (60000, 28, 28, 1)\n",
            "y_train: (60000, 10)\n",
            "X_test: (10000, 28, 28, 1)\n",
            "y_test (10000, 10)\n",
            "X_train: (67420, 28, 28, 1)\n",
            "y_train: (67420, 10)\n",
            "X_test: (10000, 28, 28, 1)\n",
            "y_test (10000, 10)\n",
            "Print noisy label generation statistics:\n",
            "Noisy class 0, has 5977 samples.\n",
            "Noisy class 1, has 6698 samples.\n",
            "Noisy class 2, has 5966 samples.\n",
            "Noisy class 3, has 6122 samples.\n",
            "Noisy class 4, has 5823 samples.\n",
            "Noisy class 5, has 5367 samples.\n",
            "Noisy class 6, has 5959 samples.\n",
            "Noisy class 7, has 6265 samples.\n",
            "Noisy class 8, has 5834 samples.\n",
            "Noisy class 9, has 5989 samples.\n",
            "X_train: (60000, 28, 28, 1)\n",
            "y_train: (60000, 10)\n",
            "X_test: (10000, 28, 28, 1)\n",
            "y_test (10000, 10)\n",
            "Print noisy label generation statistics:\n",
            "Noisy class 0, has 6621 samples.\n",
            "Noisy class 1, has 6732 samples.\n",
            "Noisy class 2, has 6696 samples.\n",
            "Noisy class 3, has 6837 samples.\n",
            "Noisy class 4, has 6869 samples.\n",
            "Noisy class 5, has 6785 samples.\n",
            "Noisy class 6, has 6736 samples.\n",
            "Noisy class 7, has 6744 samples.\n",
            "Noisy class 8, has 6659 samples.\n",
            "Noisy class 9, has 6741 samples.\n",
            "X_train: (67420, 28, 28, 1)\n",
            "y_train: (67420, 10)\n",
            "X_test: (10000, 28, 28, 1)\n",
            "y_test (10000, 10)\n",
            "X_train: (60000, 28, 28, 1)\n",
            "y_train: (60000,)\n",
            "X_test: (10000, 28, 28, 1)\n",
            "y_test (10000,)\n",
            "X_train: (67420, 28, 28, 1)\n",
            "y_train: (67420,)\n",
            "X_test: (10000, 28, 28, 1)\n",
            "y_test (10000,)\n",
            "X_train: (60000, 28, 28, 1)\n",
            "y_train: (60000,)\n",
            "X_test: (10000, 28, 28, 1)\n",
            "y_test (10000,)\n",
            "X_train: (67420, 28, 28, 1)\n",
            "y_train: (67420,)\n",
            "X_test: (10000, 28, 28, 1)\n",
            "y_test (10000,)\n",
            "Print noisy label generation statistics:\n",
            "Noisy class 0, has 5803 samples.\n",
            "Noisy class 1, has 6786 samples.\n",
            "Noisy class 2, has 5867 samples.\n",
            "Noisy class 3, has 6105 samples.\n",
            "Noisy class 4, has 5849 samples.\n",
            "Noisy class 5, has 5389 samples.\n",
            "Noisy class 6, has 6018 samples.\n",
            "Noisy class 7, has 6384 samples.\n",
            "Noisy class 8, has 5822 samples.\n",
            "Noisy class 9, has 5977 samples.\n",
            "X_train: (60000, 28, 28, 1)\n",
            "y_train: (60000,)\n",
            "X_test: (10000, 28, 28, 1)\n",
            "y_test (10000,)\n",
            "Print noisy label generation statistics:\n",
            "Noisy class 0, has 6756 samples.\n",
            "Noisy class 1, has 6748 samples.\n",
            "Noisy class 2, has 6802 samples.\n",
            "Noisy class 3, has 6623 samples.\n",
            "Noisy class 4, has 6804 samples.\n",
            "Noisy class 5, has 6779 samples.\n",
            "Noisy class 6, has 6769 samples.\n",
            "Noisy class 7, has 6757 samples.\n",
            "Noisy class 8, has 6724 samples.\n",
            "Noisy class 9, has 6658 samples.\n",
            "X_train: (67420, 28, 28, 1)\n",
            "y_train: (67420,)\n",
            "X_test: (10000, 28, 28, 1)\n",
            "y_test (10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBZ19aF_Lkyc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9ed5cf0-478f-410d-f784-176c45a6abe3"
      },
      "source": [
        "# SVM\n",
        "\n",
        "import math, time \n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score\n",
        "from sklearn.decomposition import PCA\n",
        "from google.colab import drive,files\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "for noise_type in list_of_filenames:\n",
        "  X_train, y_train, X_test, y_test = mapped_function_dict_deterministic[noise_type]\n",
        "  X_train = np.reshape(X_train,(len(X_train),784))\n",
        "  pca = PCA(n_components = 40, svd_solver='randomized',whiten=True).fit(X_train)\n",
        "  X_train = pca.transform(X_train)\n",
        "  model_linear = SVC(kernel='linear')\n",
        "  model_linear.fit(X_train, y_train)\n",
        "  X_test = np.reshape(X_test,(10000,784))\n",
        "  X_test = pca.transform(X_test)\n",
        "  y_pred = model_linear.predict(X_test)\n",
        "  output_dataframe=pd.DataFrame(y_pred, columns=['labels'])\n",
        "  csv_data = output_dataframe.to_csv()\n",
        "  with open('/content/drive/My Drive/CSE555/'+noise_type+'_svm.csv', 'w') as f:\n",
        "    print(\"Wrote\",noise_type)\n",
        "    f.write(csv_data)\n",
        "  # accuracy\n",
        "  print(\"accuracy:\", accuracy_score(y_true=y_test, y_pred=y_pred), \"\\n\")\n",
        "\n",
        "  # cm\n",
        "  print(confusion_matrix(y_true=y_test, y_pred=y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Wrote imbalanced\n",
            "accuracy: 0.9337 \n",
            "\n",
            "[[ 967    0    1    0    0    7    3    1    1    0]\n",
            " [   0 1119    4    2    0    2    0    1    7    0]\n",
            " [   8    4  950   14    9    4   13   11   19    0]\n",
            " [   1    2   18  934    3   25    0   11   11    5]\n",
            " [   2    0    6    1  937    2    6    3    2   23]\n",
            " [   8    6    8   51    5  783    8    1   16    6]\n",
            " [   8    3    9    2    7   14  914    0    1    0]\n",
            " [   1   11   26    8    8    0    0  957    3   14]\n",
            " [   6    5    9   27    7   29   11    5  872    3]\n",
            " [   5    8    4   12   34    6    1   25   10  904]]\n",
            "Wrote balanced\n",
            "accuracy: 0.9345 \n",
            "\n",
            "[[ 966    0    1    0    0    7    4    1    1    0]\n",
            " [   0 1119    3    3    0    3    0    1    6    0]\n",
            " [   7    3  948   14    8    5   15   12   19    1]\n",
            " [   0    1   18  932    2   27    1    9   14    6]\n",
            " [   1    0    8    1  938    2    6    3    2   21]\n",
            " [   8    4    6   46    6  790   10    2   16    4]\n",
            " [   9    1    9    2    7   11  918    0    1    0]\n",
            " [   1   10   27    5    8    0    0  960    4   13]\n",
            " [   5    4   11   27    7   31    8    5  873    3]\n",
            " [   5    8    4   11   39    7    1   25    8  901]]\n",
            "Wrote imbalanced_asym\n",
            "accuracy: 0.8881 \n",
            "\n",
            "[[ 965    0    0    0    0    4    5    3    2    1]\n",
            " [   0 1108    3    3    0    3    4    0   14    0]\n",
            " [   8    4  839    4    9    8   13  105   40    2]\n",
            " [   4   19    8  816    0   20    5   20  114    4]\n",
            " [   2    5    4    0  926    3   12    4    1   25]\n",
            " [  12   13    4   34    7  715   38    8   47   14]\n",
            " [  16    5    5    0   19   13  889    5    6    0]\n",
            " [   2   96    4    3   11    4    0  865    8   35]\n",
            " [  10   12    4    4   10   42   17    9  859    7]\n",
            " [   7   20    1    4   34   10    2   17   15  899]]\n",
            "Wrote balanced_asym\n",
            "accuracy: 0.8891 \n",
            "\n",
            "[[ 966    0    0    0    0    6    3    1    2    2]\n",
            " [   0 1103    3    3    0    4    5    0   17    0]\n",
            " [   7    2  852    5   12   12    8   94   38    2]\n",
            " [   5   19   10  821    0   21    2   17  111    4]\n",
            " [   1    4    4    0  929    7    9    2    2   24]\n",
            " [  10   12    2   35    8  739   26    8   40   12]\n",
            " [  18    5    7    0   18   45  852    5    8    0]\n",
            " [   2   89    5    2   10    4    0  868    8   40]\n",
            " [   9   13    3    3    9   43   17    9  862    6]\n",
            " [   5   21    2    5   36    9    2   17   13  899]]\n",
            "Wrote imbalanced_sym\n",
            "accuracy: 0.9073 \n",
            "\n",
            "[[ 955    0    0    1    2   11    8    1    2    0]\n",
            " [   0 1114    1    5    0    3    3    0    9    0]\n",
            " [  11   21  876   17   16    3   23   23   35    7]\n",
            " [   2    4   15  916    1   23    4   14   25    6]\n",
            " [   1    3    3    0  917    1   10    1    3   43]\n",
            " [  13    7    6   57   12  742   19   11   16    9]\n",
            " [   9    4    3    0   13   12  914    1    2    0]\n",
            " [   2   30   17    3   13    4    1  931    3   24]\n",
            " [  11   20    3   18   12   45   15   15  829    6]\n",
            " [   7    9    1   12   57   12    1   20   11  879]]\n",
            "Wrote balanced_sym\n",
            "accuracy: 0.9093 \n",
            "\n",
            "[[ 955    0    0    1    4   11    6    1    2    0]\n",
            " [   0 1108    1    6    0    3    3    1   13    0]\n",
            " [   8   12  890   14   21    4   19   22   36    6]\n",
            " [   4    3   17  910    0   31    4   10   23    8]\n",
            " [   1    3    3    0  908    1    8    1    4   53]\n",
            " [  11    7    4   40   15  770   22    3   13    7]\n",
            " [  14    4    6    0   10   11  911    0    2    0]\n",
            " [   2   26   16    4   16    4    1  930    3   26]\n",
            " [  10   21    6   17   10   44   12   14  832    8]\n",
            " [  12   10    2   13   60    8    1   16    8  879]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvaaUc6DLu4E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66988c28-8e09-4af2-fbc1-b491d4ae4c1f"
      },
      "source": [
        "# Logistic\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score\n",
        "for noise_type in list_of_filenames:\n",
        "  X_train, y_train, X_test, y_test = mapped_function_dict_deterministic[noise_type]\n",
        "  X_train = np.reshape(X_train,(len(X_train),784))\n",
        "  logisticRegr = LogisticRegression()\n",
        "\n",
        "  logisticRegr.fit(X_train, y_train)\n",
        "\n",
        "  X_test = np.reshape(X_test,(10000,784))\n",
        "\n",
        "  y_pred = logisticRegr.predict(X_test)\n",
        "  output_dataframe=pd.DataFrame(y_pred, columns=['labels'])\n",
        "  csv_data = output_dataframe.to_csv()\n",
        "  with open('/content/drive/My Drive/CSE555/'+noise_type+'_logistic.csv', 'w') as f:\n",
        "    print(\"Wrote\",noise_type)\n",
        "    f.write(csv_data)\n",
        "  # accuracy\n",
        "  print(\"accuracy:\", accuracy_score(y_true=y_test, y_pred=y_pred), \"\\n\")\n",
        "\n",
        "  # cm\n",
        "  print(confusion_matrix(y_true=y_test, y_pred=y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote imbalanced\n",
            "accuracy: 0.9256 \n",
            "\n",
            "[[ 959    0    0    3    1    7    5    4    1    0]\n",
            " [   0 1112    4    2    0    2    3    2   10    0]\n",
            " [   6    9  928   16    8    4   15    7   35    4]\n",
            " [   4    1   17  921    0   23    4   11   23    6]\n",
            " [   1    1    7    4  914    0   10    4   10   31]\n",
            " [  10    2    3   37    8  779   14    5   29    5]\n",
            " [   9    3    7    3    8   15  910    2    1    0]\n",
            " [   1    9   23    6    7    1    0  950    2   29]\n",
            " [   9   10    8   26    8   26   12    7  857   11]\n",
            " [   9    8    0   11   23    6    0   19    7  926]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote balanced\n",
            "accuracy: 0.926 \n",
            "\n",
            "[[ 961    0    1    3    0    6    4    4    1    0]\n",
            " [   0 1110    4    1    0    1    5    2   12    0]\n",
            " [   5   10  923   18    9    3   15    7   36    6]\n",
            " [   3    1   16  923    2   23    3   11   21    7]\n",
            " [   1    2    5    3  916    0   10    4   10   31]\n",
            " [   8    2    2   35    8  788   14    3   28    4]\n",
            " [  10    3    7    1    8   16  910    2    1    0]\n",
            " [   1    8   20    7    8    2    0  946    3   33]\n",
            " [   8    8    6   22    9   27   10    7  863   14]\n",
            " [  11    7    1    9   27    6    0   21    7  920]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote imbalanced_asym\n",
            "accuracy: 0.8038 \n",
            "\n",
            "[[ 957    0    1    0    1    8    8    4    1    0]\n",
            " [   0 1100    1    2    1    4    3    3   21    0]\n",
            " [  11   12  659    8    9   11   14  208   93    7]\n",
            " [   3   18    9  655    1   22   12   27  259    4]\n",
            " [   1    7    1    1  904    2   13    8    4   41]\n",
            " [  11   14    1   17   15  591  134   18   84    7]\n",
            " [  13    7    6    0   18  119  772   11   12    0]\n",
            " [   4  298    5    3   12    1    2  625   13   65]\n",
            " [   7   22    1    3   15   28   19   10  862    7]\n",
            " [   8   20    0    1   25    5    2   11   24  913]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote balanced_asym\n",
            "accuracy: 0.8143 \n",
            "\n",
            "[[ 960    0    1    0    1   11    4    2    1    0]\n",
            " [   0 1095    2    3    1    3    5    2   24    0]\n",
            " [  10   13  671    3   12   17   13  189   99    5]\n",
            " [   2   14    6  684    1   23    8   25  245    2]\n",
            " [   1    5    0    0  906    8    8    6    8   40]\n",
            " [  12   12    0   14   14  651   84   20   77    8]\n",
            " [  16    6    4    0   19  116  772   13   11    1]\n",
            " [   5  290    5    0   15    2    1  631    9   70]\n",
            " [  11   17    1    2   15   27   17   12  863    9]\n",
            " [   9   18    0    3   31    3    2   10   23  910]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote imbalanced_sym\n",
            "accuracy: 0.8765 \n",
            "\n",
            "[[ 930    1    2    3    5   10   18    4    6    1]\n",
            " [   0 1105    2    2    1    2    4    2   17    0]\n",
            " [  15   44  831   25   15    3   22   23   44   10]\n",
            " [   3    9   22  876    4   31    7   24   23   11]\n",
            " [   0   12    9    2  890    2   10    1    9   47]\n",
            " [  10   13    4   46   16  718   19   22   25   19]\n",
            " [  11   12   11    3   20   25  870    0    6    0]\n",
            " [   3   27   12   10   23    4    2  895    2   50]\n",
            " [  12   25   11   26   21   32   16   15  796   20]\n",
            " [  11   11    2   13   55   11    3   41    8  854]]\n",
            "Wrote balanced_sym\n",
            "accuracy: 0.8872 \n",
            "\n",
            "[[ 933    1    2    2    4   11   12    3   11    1]\n",
            " [   0 1099    2    5    1    2    4    1   21    0]\n",
            " [  11   28  855   21   20    5   18   17   48    9]\n",
            " [   5    5   19  885    4   36    7   11   25   13]\n",
            " [   0    8    6    2  900    2   14    2    7   41]\n",
            " [  13   10    1   24   14  762   18    9   29   12]\n",
            " [  10    8    6    0   14   25  893    0    2    0]\n",
            " [   5   26   17    5   23    1    2  882    4   63]\n",
            " [  11   27    8   19   20   34   17   16  799   23]\n",
            " [   8   11    3   11   64    9    2   30    7  864]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3G3pVXtBEF6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d7dd12d-778c-4ca4-b570-f88f876728a9"
      },
      "source": [
        "# SL\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import keras.backend as K\n",
        "import argparse\n",
        "from sklearn.decomposition import PCA\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import load_model\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from keras.layers import Input, Conv2D, Dense, MaxPooling2D, Dropout, Flatten, Activation, BatchNormalization\n",
        "from keras.models import Model\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.callbacks import CSVLogger\n",
        "\n",
        "def symmetric_cross_entropy(alpha, beta):\n",
        "    def loss(y_true, y_pred):\n",
        "        y_true_1 = y_true\n",
        "        y_pred_1 = y_pred\n",
        "\n",
        "        y_true_2 = y_true\n",
        "        y_pred_2 = y_pred\n",
        "\n",
        "        y_pred_1 = tf.clip_by_value(y_pred_1, 1e-7, 1.0)\n",
        "        y_true_2 = tf.clip_by_value(y_true_2, 1e-4, 1.0)\n",
        "\n",
        "        return alpha*tf.reduce_mean(-tf.reduce_sum(y_true_1 * tf.math.log(y_pred_1), axis = -1)) + beta*tf.reduce_mean(-tf.reduce_sum(y_pred_2 * tf.math.log(y_true_2), axis = -1))\n",
        "    return loss\n",
        "\n",
        "for noise_type in list_of_filenames: # Replace with whatever slice was run last\n",
        "  X_train, y_train, X_test, y_test = mapped_function_dict[noise_type]\n",
        "  image_shape = X_train.shape[1:]\n",
        "  csv_logger = CSVLogger('/content/drive/MyDrive/CSE555/log_SE_'+noise_type+'.csv', append=True, separator=';')\n",
        "  img_input = Input(shape=image_shape)\n",
        "\n",
        "  x = Conv2D(32, (3, 3), padding='same', kernel_initializer=\"he_normal\", name='conv1')(img_input)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = MaxPooling2D((2, 2), strides=(2, 2), name='pool1')(x)\n",
        "\n",
        "  x = Conv2D(64, (3, 3), padding='same', kernel_initializer=\"he_normal\", name='conv2')(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = MaxPooling2D((2, 2), strides=(2, 2), name='pool2')(x)\n",
        "\n",
        "  x = Flatten()(x)\n",
        "\n",
        "  x = Dense(128, kernel_initializer=\"he_normal\", name='fc1')(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu', name='lid')(x)\n",
        "  # x = Dropout(0.2)(x)\n",
        "\n",
        "  x = Dense(10, kernel_initializer=\"he_normal\")(x)\n",
        "  x = Activation(tf.nn.softmax)(x)\n",
        "\n",
        "  model = Model(img_input, x)\n",
        "  optimizer = SGD(lr=0.1, decay=1e-4, momentum=0.9)\n",
        "  loss = symmetric_cross_entropy(1.0,1.0)\n",
        "  model.compile(\n",
        "  loss=loss,\n",
        "  optimizer=optimizer,\n",
        "  metrics=['accuracy']\n",
        "  )\n",
        "  datagen = ImageDataGenerator()\n",
        "  datagen.fit(X_train)\n",
        "\n",
        "  model.fit_generator(datagen.flow(X_train, y_train, batch_size=128),\n",
        "                  steps_per_epoch=len(X_train) / 128, epochs=10,\n",
        "                  validation_data=(X_test, y_test),\n",
        "                  verbose=1,callbacks=[csv_logger]\n",
        "                  )\n",
        "  output_test = model.predict_on_batch(X_test)\n",
        "  labels = np.argmax(output_test, axis=1)\n",
        "  output_dataframe = pd.DataFrame(labels)\n",
        "  csv_data = output_dataframe.to_csv()\n",
        "  with open('/content/drive/My Drive/CSE555/'+noise_type+'_SE-CNN_predictions.csv', 'w') as f:\n",
        "    f.write(csv_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:76: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "468/468 [==============================] - 96s 202ms/step - loss: 0.4412 - accuracy: 0.9698 - val_loss: 0.2629 - val_accuracy: 0.9821\n",
            "Epoch 2/10\n",
            "468/468 [==============================] - 95s 203ms/step - loss: 0.1670 - accuracy: 0.9891 - val_loss: 0.1530 - val_accuracy: 0.9892\n",
            "Epoch 3/10\n",
            "468/468 [==============================] - 95s 202ms/step - loss: 0.1219 - accuracy: 0.9921 - val_loss: 0.1678 - val_accuracy: 0.9882\n",
            "Epoch 4/10\n",
            "468/468 [==============================] - 95s 202ms/step - loss: 0.0907 - accuracy: 0.9940 - val_loss: 0.1213 - val_accuracy: 0.9907\n",
            "Epoch 5/10\n",
            "468/468 [==============================] - 95s 203ms/step - loss: 0.0656 - accuracy: 0.9959 - val_loss: 0.1214 - val_accuracy: 0.9914\n",
            "Epoch 6/10\n",
            "468/468 [==============================] - 95s 202ms/step - loss: 0.0451 - accuracy: 0.9972 - val_loss: 0.1363 - val_accuracy: 0.9901\n",
            "Epoch 7/10\n",
            "468/468 [==============================] - 96s 204ms/step - loss: 0.0375 - accuracy: 0.9977 - val_loss: 0.1298 - val_accuracy: 0.9912\n",
            "Epoch 8/10\n",
            "468/468 [==============================] - 96s 204ms/step - loss: 0.0308 - accuracy: 0.9980 - val_loss: 0.1087 - val_accuracy: 0.9926\n",
            "Epoch 9/10\n",
            "468/468 [==============================] - 95s 203ms/step - loss: 0.0220 - accuracy: 0.9988 - val_loss: 0.1045 - val_accuracy: 0.9929\n",
            "Epoch 10/10\n",
            "468/468 [==============================] - 96s 204ms/step - loss: 0.0190 - accuracy: 0.9988 - val_loss: 0.1103 - val_accuracy: 0.9925\n",
            "Epoch 1/10\n",
            "526/526 [==============================] - 108s 204ms/step - loss: 0.4405 - accuracy: 0.9698 - val_loss: 0.2748 - val_accuracy: 0.9816\n",
            "Epoch 2/10\n",
            "526/526 [==============================] - 106s 202ms/step - loss: 0.1766 - accuracy: 0.9882 - val_loss: 0.1935 - val_accuracy: 0.9852\n",
            "Epoch 3/10\n",
            "526/526 [==============================] - 106s 202ms/step - loss: 0.1192 - accuracy: 0.9922 - val_loss: 0.1170 - val_accuracy: 0.9916\n",
            "Epoch 4/10\n",
            "526/526 [==============================] - 107s 203ms/step - loss: 0.0883 - accuracy: 0.9943 - val_loss: 0.1127 - val_accuracy: 0.9923\n",
            "Epoch 5/10\n",
            "526/526 [==============================] - 106s 202ms/step - loss: 0.0642 - accuracy: 0.9959 - val_loss: 0.1368 - val_accuracy: 0.9902\n",
            "Epoch 6/10\n",
            "526/526 [==============================] - 107s 203ms/step - loss: 0.0556 - accuracy: 0.9964 - val_loss: 0.1621 - val_accuracy: 0.9882\n",
            "Epoch 7/10\n",
            "526/526 [==============================] - 106s 201ms/step - loss: 0.0387 - accuracy: 0.9976 - val_loss: 0.1020 - val_accuracy: 0.9927\n",
            "Epoch 8/10\n",
            "526/526 [==============================] - 106s 202ms/step - loss: 0.0318 - accuracy: 0.9981 - val_loss: 0.1229 - val_accuracy: 0.9912\n",
            "Epoch 9/10\n",
            "526/526 [==============================] - 107s 203ms/step - loss: 0.0206 - accuracy: 0.9988 - val_loss: 0.1064 - val_accuracy: 0.9925\n",
            "Epoch 10/10\n",
            "526/526 [==============================] - 107s 203ms/step - loss: 0.0180 - accuracy: 0.9989 - val_loss: 0.1322 - val_accuracy: 0.9912\n",
            "Epoch 1/10\n",
            "468/468 [==============================] - 96s 204ms/step - loss: 2.9239 - accuracy: 0.7659 - val_loss: 1.2596 - val_accuracy: 0.9554\n",
            "Epoch 2/10\n",
            "468/468 [==============================] - 95s 202ms/step - loss: 2.6237 - accuracy: 0.7875 - val_loss: 1.4883 - val_accuracy: 0.9073\n",
            "Epoch 3/10\n",
            "468/468 [==============================] - 95s 202ms/step - loss: 2.5492 - accuracy: 0.7923 - val_loss: 1.0722 - val_accuracy: 0.9462\n",
            "Epoch 4/10\n",
            "468/468 [==============================] - 94s 201ms/step - loss: 2.5142 - accuracy: 0.7941 - val_loss: 1.2576 - val_accuracy: 0.9155\n",
            "Epoch 5/10\n",
            "468/468 [==============================] - 94s 201ms/step - loss: 2.4706 - accuracy: 0.7970 - val_loss: 1.0289 - val_accuracy: 0.9662\n",
            "Epoch 6/10\n",
            "468/468 [==============================] - 95s 203ms/step - loss: 2.4287 - accuracy: 0.7997 - val_loss: 1.1632 - val_accuracy: 0.9643\n",
            "Epoch 7/10\n",
            "468/468 [==============================] - 94s 202ms/step - loss: 2.3702 - accuracy: 0.8062 - val_loss: 1.5660 - val_accuracy: 0.9115\n",
            "Epoch 8/10\n",
            "468/468 [==============================] - 95s 203ms/step - loss: 2.3185 - accuracy: 0.8113 - val_loss: 1.7389 - val_accuracy: 0.9010\n",
            "Epoch 9/10\n",
            "468/468 [==============================] - 96s 204ms/step - loss: 2.2431 - accuracy: 0.8176 - val_loss: 1.6061 - val_accuracy: 0.8963\n",
            "Epoch 10/10\n",
            "468/468 [==============================] - 96s 204ms/step - loss: 2.1620 - accuracy: 0.8243 - val_loss: 1.7277 - val_accuracy: 0.8754\n",
            "Epoch 1/10\n",
            "526/526 [==============================] - 109s 204ms/step - loss: 2.9595 - accuracy: 0.7651 - val_loss: 1.0239 - val_accuracy: 0.9763\n",
            "Epoch 2/10\n",
            "526/526 [==============================] - 107s 203ms/step - loss: 2.6447 - accuracy: 0.7858 - val_loss: 0.8611 - val_accuracy: 0.9809\n",
            "Epoch 3/10\n",
            "526/526 [==============================] - 107s 204ms/step - loss: 2.5751 - accuracy: 0.7901 - val_loss: 0.9502 - val_accuracy: 0.9764\n",
            "Epoch 4/10\n",
            "526/526 [==============================] - 108s 205ms/step - loss: 2.5276 - accuracy: 0.7927 - val_loss: 1.2900 - val_accuracy: 0.9478\n",
            "Epoch 5/10\n",
            "526/526 [==============================] - 107s 203ms/step - loss: 2.4833 - accuracy: 0.7967 - val_loss: 1.5806 - val_accuracy: 0.9169\n",
            "Epoch 6/10\n",
            "526/526 [==============================] - 107s 203ms/step - loss: 2.4487 - accuracy: 0.7982 - val_loss: 1.2873 - val_accuracy: 0.9490\n",
            "Epoch 7/10\n",
            "526/526 [==============================] - 107s 202ms/step - loss: 2.3952 - accuracy: 0.8035 - val_loss: 1.4354 - val_accuracy: 0.9227\n",
            "Epoch 8/10\n",
            "526/526 [==============================] - 106s 202ms/step - loss: 2.3360 - accuracy: 0.8106 - val_loss: 1.3427 - val_accuracy: 0.9355\n",
            "Epoch 9/10\n",
            "526/526 [==============================] - 106s 202ms/step - loss: 2.2594 - accuracy: 0.8176 - val_loss: 1.5145 - val_accuracy: 0.8997\n",
            "Epoch 10/10\n",
            "526/526 [==============================] - 106s 202ms/step - loss: 2.1800 - accuracy: 0.8236 - val_loss: 1.1494 - val_accuracy: 0.9293\n",
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7faac5dfe7a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Epoch 1/10\n",
            "468/468 [==============================] - 96s 203ms/step - loss: 6.6124 - accuracy: 0.5781 - val_loss: 1.6969 - val_accuracy: 0.9767\n",
            "Epoch 2/10\n",
            "468/468 [==============================] - 95s 202ms/step - loss: 6.3477 - accuracy: 0.5913 - val_loss: 0.9707 - val_accuracy: 0.9834\n",
            "Epoch 3/10\n",
            "468/468 [==============================] - 95s 202ms/step - loss: 6.2775 - accuracy: 0.5934 - val_loss: 1.0615 - val_accuracy: 0.9873\n",
            "Epoch 4/10\n",
            "468/468 [==============================] - 96s 204ms/step - loss: 6.2153 - accuracy: 0.5952 - val_loss: 1.0369 - val_accuracy: 0.9862\n",
            "Epoch 5/10\n",
            "468/468 [==============================] - 95s 203ms/step - loss: 6.1748 - accuracy: 0.5962 - val_loss: 1.0341 - val_accuracy: 0.9875\n",
            "Epoch 6/10\n",
            "468/468 [==============================] - 95s 202ms/step - loss: 6.1328 - accuracy: 0.5975 - val_loss: 0.9787 - val_accuracy: 0.9889\n",
            "Epoch 7/10\n",
            "468/468 [==============================] - 95s 203ms/step - loss: 6.0817 - accuracy: 0.5985 - val_loss: 1.0545 - val_accuracy: 0.9846\n",
            "Epoch 8/10\n",
            "468/468 [==============================] - 95s 204ms/step - loss: 6.0307 - accuracy: 0.6002 - val_loss: 0.8840 - val_accuracy: 0.9832\n",
            "Epoch 9/10\n",
            "468/468 [==============================] - 95s 203ms/step - loss: 5.9732 - accuracy: 0.6017 - val_loss: 1.2022 - val_accuracy: 0.9832\n",
            "Epoch 10/10\n",
            "468/468 [==============================] - 95s 202ms/step - loss: 5.9267 - accuracy: 0.6036 - val_loss: 1.4435 - val_accuracy: 0.9716\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7faac2e73c20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Epoch 1/10\n",
            "526/526 [==============================] - 107s 202ms/step - loss: 6.6041 - accuracy: 0.5783 - val_loss: 1.5076 - val_accuracy: 0.9822\n",
            "Epoch 2/10\n",
            "526/526 [==============================] - 107s 204ms/step - loss: 6.3552 - accuracy: 0.5916 - val_loss: 1.8973 - val_accuracy: 0.9842\n",
            "Epoch 3/10\n",
            "526/526 [==============================] - 108s 205ms/step - loss: 6.2713 - accuracy: 0.5943 - val_loss: 0.9584 - val_accuracy: 0.9847\n",
            "Epoch 4/10\n",
            "526/526 [==============================] - 107s 204ms/step - loss: 6.2156 - accuracy: 0.5960 - val_loss: 1.1374 - val_accuracy: 0.9861\n",
            "Epoch 5/10\n",
            "526/526 [==============================] - 107s 203ms/step - loss: 6.1748 - accuracy: 0.5967 - val_loss: 0.8210 - val_accuracy: 0.9889\n",
            "Epoch 6/10\n",
            "526/526 [==============================] - 106s 201ms/step - loss: 6.1053 - accuracy: 0.5984 - val_loss: 1.4098 - val_accuracy: 0.9808\n",
            "Epoch 7/10\n",
            "526/526 [==============================] - 106s 201ms/step - loss: 6.0644 - accuracy: 0.5990 - val_loss: 1.1928 - val_accuracy: 0.9853\n",
            "Epoch 8/10\n",
            "526/526 [==============================] - 107s 203ms/step - loss: 5.9996 - accuracy: 0.6012 - val_loss: 1.4313 - val_accuracy: 0.9735\n",
            "Epoch 9/10\n",
            "526/526 [==============================] - 107s 203ms/step - loss: 5.9406 - accuracy: 0.6038 - val_loss: 1.1032 - val_accuracy: 0.9758\n",
            "Epoch 10/10\n",
            "526/526 [==============================] - 107s 203ms/step - loss: 5.8643 - accuracy: 0.6069 - val_loss: 1.2047 - val_accuracy: 0.9712\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTMBzO0-XTOV"
      },
      "source": [
        "# LDAM\n",
        "# NOTE: Make sure the file structuring is as described in the path below and Drive is linked!\n",
        "# Also, the outputs are saved only if there is a certain file structure in Drive.\n",
        "# Refer to the project report's LDAM section for instructions\n",
        "\n",
        "!python  /content/drive/My\\ Drive/CSE555/LDAM-DRW-master/cifar_train.py --imb_type exp --imb_factor 0.01 --loss_type LDAM --train_rule DRW"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 762
        },
        "id": "8C2_lzhQ1veL",
        "outputId": "588c587f-a28b-4b17-a565-b32979182835"
      },
      "source": [
        "# Proposed DL\n",
        "# TODO: Find and implement a loss function\n",
        "\n",
        "\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.models import Sequential,Model\n",
        "from keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Dropout, Input\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "import pandas as pd\n",
        "for noise_type in list_of_filenames:\n",
        "  x_train, y_train, x_test, y_test = mapped_function_dict[noise_type]\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(filters=32, kernel_size=(5,5), activation='relu', input_shape=x_train.shape[1:]))\n",
        "  model.add(Conv2D(filters=32, kernel_size=(5,5), activation='relu'))\n",
        "  model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(rate=0.25))\n",
        "  model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
        "  model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
        "  model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(rate=0.25))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(256, activation='relu'))\n",
        "  model.add(Dropout(rate=0.5))\n",
        "  model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "  model = Sequential([\n",
        "      Conv2D(filters=32, kernel_size=(5,5), activation='relu', input_shape=x_train.shape[1:]),\n",
        "      Conv2D(filters=32, kernel_size=(5,5), activation='relu'),\n",
        "      MaxPool2D(pool_size=(2, 2)),\n",
        "      Dropout(rate=0.25),\n",
        "      Conv2D(filters=64, kernel_size=(3,3), activation='relu'),\n",
        "      Conv2D(filters=64, kernel_size=(3,3), activation='relu'),\n",
        "      MaxPool2D(pool_size=(2, 2)),\n",
        "      Dropout(rate=0.25),\n",
        "      Flatten(),\n",
        "      Dense(256, activation='relu'),\n",
        "      Dropout(rate=0.5),\n",
        "      Dense(10, activation='softmax')\n",
        "  ])\n",
        "\n",
        "  inputs = Input(shape=x_train.shape[1:])\n",
        "\n",
        "  x = Conv2D(filters=32, kernel_size=(5,5), activation='relu')(inputs)\n",
        "  x = Conv2D(filters=32, kernel_size=(5,5), activation='relu')(x)\n",
        "  x = MaxPool2D(pool_size=(2, 2))(x)\n",
        "  x = Dropout(rate=0.25)(x)\n",
        "\n",
        "  x = Conv2D(filters=64, kernel_size=(3,3), activation='relu')(x)\n",
        "  x = Conv2D(filters=64, kernel_size=(3,3), activation='relu')(x)\n",
        "  x = MaxPool2D(pool_size=(2, 2))(x)\n",
        "  x = Dropout(rate=0.25)(x)\n",
        "\n",
        "  x = Flatten()(x)\n",
        "  x = Dense(256, activation='relu')(x)\n",
        "  x = Dropout(rate=0.5)(x)\n",
        "  predictions = Dense(10, activation='softmax')(x)\n",
        "\n",
        "  model = Model(inputs=inputs, outputs=predictions)\n",
        "  print(\"Running for\",noise_type,\"dataset:\")\n",
        "  model.compile(\n",
        "      loss='categorical_crossentropy', \n",
        "      optimizer='adam', \n",
        "      metrics=['accuracy']\n",
        "  )\n",
        "\n",
        "  # creating datagenerator for augmenting images\n",
        "  datagen = ImageDataGenerator(\n",
        "          rotation_range=10,\n",
        "          zoom_range=0.1,\n",
        "          width_shift_range=0.1,\n",
        "          height_shift_range=0.1)\n",
        "\n",
        "  epochs = 3\n",
        "  batch_size = 32\n",
        "\n",
        "  history = model.fit(datagen.flow(x_train, y_train, batch_size=batch_size), epochs=epochs,\n",
        "                                validation_data=(x_test, y_test), steps_per_epoch=x_train.shape[0]//batch_size)\n",
        "  output_test = model.predict_on_batch(x_test)\n",
        "  labels = np.argmax(output_test, axis=1)\n",
        "  output_dataframe = pd.DataFrame(labels)\n",
        "  csv_data = output_dataframe.to_csv()\n",
        "  with open('/content/drive/My Drive/CSE555/'+noise_type+'_proposed-CNN_predictions.csv', 'w') as f:\n",
        "    f.write(csv_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running for imbalanced dataset:\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-a5fed6af070a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m   history = model.fit(datagen.flow(x_train, y_train, batch_size=batch_size), epochs=epochs,\n\u001b[0;32m---> 80\u001b[0;31m                                 validation_data=(x_test, y_test), steps_per_epoch=x_train.shape[0]//batch_size)\n\u001b[0m\u001b[1;32m     81\u001b[0m   \u001b[0moutput_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m   \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1130\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 810, in train_step\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/losses.py\", line 143, in __call__\n        losses, sample_weight, reduction=self._get_reduction())\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/losses_utils.py\", line 309, in compute_weighted_loss\n        losses = tf.convert_to_tensor(losses)\n\n    TypeError: Failed to convert elements of NCEandMAE(\n      (nce): NormalizedCrossEntropy()\n      (mae): MeanAbsoluteError()\n    ) to Tensor. Consider casting elements to a supported type. See https://www.tensorflow.org/api_docs/python/tf/dtypes for supported TF dtypes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmfY8pIm6BA0"
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Proposed ML\n",
        "\n",
        "import math, time \n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score\n",
        "from sklearn.decomposition import PCA\n",
        "from google.colab import drive,files\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "for noise_type in list_of_filenames:\n",
        "  X_train, y_train, X_test, y_test = mapped_function_dict_deterministic[noise_type]\n",
        "  X_train = np.reshape(X_train,(len(X_train),784))\n",
        "  pca = PCA(n_components = 40, svd_solver='randomized',whiten=True).fit(X_train)\n",
        "  X_train = pca.transform(X_train)\n",
        "  clf = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(10, 5), random_state=1, max_iter = 1000)\n",
        "  clf.fit(X_train,y_train)\n",
        "  X_test = np.reshape(X_test,(10000,784))\n",
        "  X_test = pca.transform(X_test)\n",
        "  y_pred = clf.predict(X_test)\n",
        "  output_dataframe=pd.DataFrame(y_pred, columns=['labels'])\n",
        "  csv_data = output_dataframe.to_csv()\n",
        "  with open('/content/drive/My Drive/CSE555/'+noise_type+'_mlp.csv', 'w') as f:\n",
        "    print(\"Wrote\",noise_type)\n",
        "    f.write(csv_data)\n",
        "  # accuracy\n",
        "  print(\"accuracy:\", accuracy_score(y_true=y_test, y_pred=y_pred), \"\\n\")\n",
        "\n",
        "  # cm\n",
        "  print(confusion_matrix(y_true=y_test, y_pred=y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23CwH1s9sVk_",
        "outputId": "b55c6407-6f48-43d5-ec3d-7a3c78f8f7b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.9259 \n",
            "\n",
            "[[ 951    0    9    0    2    5    9    2    2    0]\n",
            " [   0 1116    4    2    0    0    4    1    8    0]\n",
            " [  17    7  935   12   10    3   16   16   14    2]\n",
            " [   1    7   16  914    2   37    1   11   15    6]\n",
            " [   1    4    4    0  910    0   14    4    6   39]\n",
            " [  13    1    8   44    0  784   16    2   14   10]\n",
            " [   6    3   10    0   13   12  905    2    7    0]\n",
            " [   3    7   23    7    9    1    0  950    4   24]\n",
            " [  10   10    7   15    8   20   18    5  864   17]\n",
            " [   9    9    1    4   21    6    1    9   19  930]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.9305 \n",
            "\n",
            "[[ 950    0    7    0    1    4   13    1    4    0]\n",
            " [   0 1111    1    6    1    1    1    1   12    1]\n",
            " [  15    4  953   10    5    5   11   14   13    2]\n",
            " [   0    3   19  928    1   25    0   12   15    7]\n",
            " [   1    7    1    0  923    1   13    1    3   32]\n",
            " [  10    0    6   31    2  804   13    1   15   10]\n",
            " [  12    2    2    0    8   14  913    0    6    1]\n",
            " [   5    8   17    6    3    0    0  946    3   40]\n",
            " [   4    8   11   14    9   28   12    6  873    9]\n",
            " [   4    7    0    7   44   12    1   21    9  904]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.8556 \n",
            "\n",
            "[[ 960    0    0    1    1    6    5    2    3    2]\n",
            " [   0 1111    4    1    0    3    2    1   11    2]\n",
            " [  13    5  786    5   14    4    9  160   35    1]\n",
            " [   7    2    6  535    0   31    1   13  404   11]\n",
            " [   1    2    1    0  926    0   11    5    3   33]\n",
            " [  10    2    3   17    4  718   84    1   44    9]\n",
            " [  22    3    6    1   13   62  842    0    8    1]\n",
            " [   6   73    6    0    8    0    1  888   14   32]\n",
            " [   3    7    6   17   11   13   12   11  878   16]\n",
            " [   9   11    0    3   36    5    3    4   26  912]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.8374 \n",
            "\n",
            "[[ 957    0    0    0    0   11    4    1    3    4]\n",
            " [   0 1113    3    1    2    7    0    0    8    1]\n",
            " [   9    3  676    7   11    6   11  270   38    1]\n",
            " [   0    0   11  566    0   22    0   23  380    8]\n",
            " [   1    5    2    0  919    0    8    5    3   39]\n",
            " [  20    2    5   17    3  685  106    4   45    5]\n",
            " [  10    3    5    0    7  147  781    3    2    0]\n",
            " [   5  101    2    0    7    1    1  866   14   31]\n",
            " [   4    2    3   10   19   21    9    6  892    8]\n",
            " [  10    6    0    3   46    0    0    4   21  919]]\n",
            "accuracy: 0.8973 \n",
            "\n",
            "[[ 954    2    4    2    0    6    8    2    2    0]\n",
            " [   0 1113    6    5    0    2    2    1    6    0]\n",
            " [  13    5  887   20   10    7   32   15   42    1]\n",
            " [   4    7   20  885    4   42    1   10   35    2]\n",
            " [   6    6    1    0  870    0   28    0    2   69]\n",
            " [  12    3    7   54    4  755   19    9   24    5]\n",
            " [  17    6   16    0   12   29  875    0    3    0]\n",
            " [   1   27   20    2    2    5    5  944    2   20]\n",
            " [   4   17   17   25    5   33   14   22  824   13]\n",
            " [   8    9    0   11   48   21    1   28   17  866]]\n",
            "accuracy: 0.8995 \n",
            "\n",
            "[[ 941    0    5    7    3   12    8    2    2    0]\n",
            " [   0 1109    3    4    1    3    2    1   11    1]\n",
            " [  10    7  898   37   20    2   17   18   18    5]\n",
            " [   3    3   21  913    0   28    3   11   23    5]\n",
            " [   1    0    3    0  905    3   25    6    5   34]\n",
            " [  20    1    2   32    8  769   20    2   27   11]\n",
            " [  22    2    8    6   25   16  866    1   12    0]\n",
            " [   6   13   42    4   14    4    0  917    3   25]\n",
            " [   7    6    7   34   12   20   46    2  810   30]\n",
            " [   7    9    2   12   62   13    0   23   14  867]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "qCKkAQlltdoi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}